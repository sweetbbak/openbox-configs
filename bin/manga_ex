#!/bin/bash

# Domain to be searched
DOMAIN='https://readm.org'
#make temp dir to store info and images
mkdir -p '/tmp/manga/'
# curl website to see if we get a response in the form of json string {"success":0}

curl -s "$DOMAIN/service/search" \
  -H 'x-requested-with: XMLHttpRequest' \
  --data-raw "dataType=json&phrase=$@" > /tmp/manga/search.json

# check and see if we had a successful search
if [[ "$(jq '.success' /tmp/manga/search.json)" != "1" ]]; then printf "Searching for $@ failed. No results."; exit; fi

# Clean results
jq '.manga[] | "\(.title)"' /tmp/manga/search.json | sed 's/"//g;s/\\//g;s/~/"/g' > /tmp/manga/results.list

# SHOW ALL RESULTS --------------------------------------------
# iterator=1; while read result; 	do printf "[$iterator] $result\n"; ((iterator=iterator+1)); done < /tmp/manga/results.list

# SELECT RESULT -----------------------------------------------
# read -p "Select what you want to read! : "  option;
# printf "\nYou have selected $option option\n"
option=$(grep -n '' /tmp/manga/results.list | fzf --reverse | cut -d":" -f1)

printf "\nYou have selected $option option\n"

((mangaoptionindex=option-1))

MANGAID=$(jq ".manga[$mangaoptionindex].url" /tmp/manga/search.json | sed 's/\/manga\///g; s/"//g')

printf "\nweburl is : $DOMAIN/manga/$MANGAID\n"

# GET FIRST CHAPTER NAME --------------------------------------
FIRSTCHAPTER=$(curl -s "$DOMAIN/manga/$MANGAID" | pup '.media-meta table.ui td:last-child .truncate a attr{href}' | cut -d/ -f4)

# GET CHAPTER LIST --------------------------------------------
curl -s "$DOMAIN/manga/$MANGAID/$FIRSTCHAPTER/all-pages" > /tmp/manga/chapterdata
cat /tmp/manga/chapterdata | pup 'img.scroll-down attr{src}' > /tmp/manga/url.list
cat /tmp/manga/chapterdata | pup '#content .ui .menu.left' > /tmp/manga/list
grep 'Chapter ' /tmp/manga/list | sed 's/Chapter //' | tac > /tmp/manga/chapter.list
grep 'Page ' /tmp/manga/list > /tmp/manga/page.list

# SHOW CHAPTER LIST -------------------------------------------
# while read chapter; do	printf "[$chapter]\n"; done < /tmp/manga/chapter.list

# SELECT CHAPTER LIST --------------------------------------------
# read -p "Download which chapter: " CHAPTERID

CHAPTERID=$(cat /tmp/manga/chapter.list | fzf --reverse | cut -d":" -f1)


rm /tmp/manga/chapter/*
mkdir -p "/tmp/manga/chapter"
cd /tmp/manga/chapter
urls="$(cat /tmp/manga/url.list | wc -l)"
printf "Total $urls images\nDownloading\t";
while read url
do
	curl -sO "$DOMAIN/$url" && printf "=";
done < /tmp/manga/url.list
cat /tmp/manga/url.list
printf "\n\n"

img2pdf "$(\ls -v /tmp/manga/chapter)" -o "$MANGAID-$CHAPTERID.pdf"
mv "$MANGAID-$CHAPTERID.pdf" ~/manga
sxiv -t $(\ls -tr *.*)

while true;
do
	printf "\n[1] Download next chapter?\n[2] Exit\n"
	read OPTION
	if [[ "$OPTION" ==  "2" ]]; then
		exit
	elif [[ "$OPTION" == "1" ]]; then
		CHAPTERID=$(cat /tmp/manga/chapter.list | awk "/$CHAPTERID/{getline;print;}" | sed 's/ //g')
		echo $CHAPTERID
		curl -s "$DOMAIN/manga/$MANGAID/$CHAPTERID/all-pages" | pup 'img.scroll-down attr{src}' > /tmp/manga/url.list
		rm /tmp/manga/chapter/*
		mkdir -p "/tmp/manga/chapter"
		cd /tmp/manga/chapter
		while read url
		do
			curl -sO "$DOMAIN/$url" && printf "\n$url downloaded."
		done < /tmp/manga/url.list
		sxiv -t $(\ls -tr *.*)
	fi
done
